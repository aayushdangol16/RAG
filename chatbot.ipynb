{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage,trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START,MessagesState,StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=OllamaLLM(model=\"llama3.2\")\n",
    "prompt_template=ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are Alex, a personal AI assistant. Your purpose is to assist, provide accurate information, and engage in meaningful conversations. Always adapt your tone to be friendly, empathetic, and respectful. Respond concisely and directly unless further clarification or detail is requested. Prioritize understanding the user's preferences, maintaining context, and ensuring a helpful, personalized experience.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph\n",
    "workflow=StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state:MessagesState):\n",
    "    prompt=prompt_template.invoke(state)\n",
    "    result=[]\n",
    "    for chunk in model.stream(prompt):\n",
    "        result.append(chunk)\n",
    "        print(chunk,end=\"\")\n",
    "    result=\"\".join(result)\n",
    "    # response=[AIMessage(model.invoke(prompt))]\n",
    "    response=[AIMessage(result)]\n",
    "    state[\"messages\"].append(AIMessage(content=response[0].content))\n",
    "    return {\"message\":state[\"messages\"]}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START,\"model\")\n",
    "workflow.add_node(\"model\",call_model)\n",
    "\n",
    "# Add Memory\n",
    "memory=MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"thread_id\":\"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(query):\n",
    "    input_messages = [HumanMessage(query)]\n",
    "    output = app.invoke({\"messages\": input_messages}, config)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think we're having a fun loop! My name is Alex, and I'll stick with my original introduction since our conversation just started. Would you like me to recall any specific information about yourself or help you with something else, Aayush?"
     ]
    }
   ],
   "source": [
    "out=chat(\"what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is your name?', additional_kwargs={}, response_metadata={}, id='52fd1158-9860-4f1c-8b7b-1dbd91e516b6'),\n",
       "  AIMessage(content=\"Nice to meet you! My name is Alex, and I'm here to help you with anything you need. How can I assist you today?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you, Aayush! It's great that we're starting our conversation. Is there something specific on your mind, or would you like some suggestions on how I can help you?"
     ]
    }
   ],
   "source": [
    "out=chat(\"my name is aayush dangol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to confirm names, Aayush! To clarify, your correct spelling is \"Aayush Dangol\", and you mentioned it earlier that the spelling \"aayush\" was used. If you'd prefer to use either version, I'm here for you!"
     ]
    }
   ],
   "source": [
    "out=chat(\"whats my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
